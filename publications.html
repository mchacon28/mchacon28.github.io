<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Publications - Academic Portfolio</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="layout-container">
        <!-- Main content -->
        <main class="main-content">
            <section id="publications" class="content-section">
                <h2 class="section-title">Publications</h2>
                <div class="section-content">
                    <div class="publication-item">
                        <h3 class="pub-title">BAM-SLDK: Biologically inspired Attention
                                                Mechanism with Spiking Learnable Delayed Kernel synapses</h3>
                        <p class="pub-authors">Mario Chacón-Falcón, Alberto Patiño-Saucedo, Luis
Camuñas-Mesa, Teresa Serrano-Gotarredona, Bernabé Linares-Barranco.</p>
                        <p class="pub-journal">IOP Neuromorphic Computing and Engineering, 2025</p>
                        <p class="pub-abstract">Spiking Neural Networks (SNNs) are emerging as an alternative neural network model due to their biological plausibility, energy efficiency, and built-in ability to learn from temporal dynamics. However, in order to effectively process data with rich spatial and temporal dependencies, the usual static projections (feedforward and recurrent) among layers of spiking neurons fail to represent all the information needed. Inspired by how synaptic delays affect the learning process in biological neurons, in this paper, we propose a biologically inspired attention mechanism based on spiking convolutions with learnable delayed kernel synapses. The proposed model increases temporal learning ability, attending simultaneously to spatial and temporal dynamics with few parameters required.</p>
                        <div class="pub-links">
                            <a href="BAMSLDKVisual.html" class="pub-link"><i class="fas fa-file-pdf"></i> Visual</a>
                            <a href="https://github.com/mchacon28/DelayedConv" class="pub-link"><i class="fas fa-code"></i> Code</a>
                            <a href="https://doi.org/10.1088/2634-4386/addb6c" class="pub-link"><i class="fas fa-link"></i> DOI</a>
                        </div>
                    </div>
                    <!--
                    <div class="publication-item">
                        <h3 class="pub-title">Topological Deep Learning: A Novel Approach to Graph Neural Networks</h3>
                        <p class="pub-authors">Last, F., Smith, J., & Johnson, R.</p>
                        <p class="pub-journal">Journal of Machine Learning Research, 2023</p>
                        <p class="pub-abstract">We present a novel approach to graph neural networks that incorporates topological information to improve performance on complex graph structures.</p>
                        <div class="pub-links">
                            <a href="#" class="pub-link"><i class="fas fa-file-pdf"></i> PDF</a>
                            <a href="#" class="pub-link"><i class="fas fa-code"></i> Code</a>
                            <a href="#" class="pub-link"><i class="fas fa-link"></i> DOI</a>
                        </div>
                    </div>
                    -->
                    <!--
                    <div class="publication-item">
                        <h3 class="pub-title">Homological Persistence in Deep Learning: Theory and Applications</h3>
                        <p class="pub-authors">Last, F., & Brown, A.</p>
                        <p class="pub-journal">Proceedings of the International Conference on Machine Learning, 2022</p>
                        <p class="pub-abstract">This paper explores the theoretical foundations of using homological persistence in deep learning architectures and demonstrates its practical applications.</p>
                        <div class="pub-links">
                            <a href="#" class="pub-link"><i class="fas fa-file-pdf"></i> PDF</a>
                            <a href="#" class="pub-link"><i class="fas fa-code"></i> Code</a>
                            <a href="#" class="pub-link"><i class="fas fa-link"></i> DOI</a>
                        </div>
                    </div>
                    -->
                </div>
            </section>
        </main>
    </div>

    <footer>
        <div class="footer-container">
            <p>&copy; 2025 First Last. Last updated: May 2025</p>
        </div>
    </footer>

    <script src="js/sidebar.js"></script>
</body>
</html> 